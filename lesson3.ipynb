{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a88f91d",
   "metadata": {},
   "source": [
    "# Functions, tools and agents\n",
    "\n",
    "We're going to improve two of the demos in this tutorial with the use of what you've just learnt in this section:\n",
    "\n",
    "- Structured outputs\n",
    "- Tool calls\n",
    "\n",
    "In the last section, we built a basic RAG bot with a search using embeddings. In the data set, there is a price field. \n",
    "\n",
    "Embedding models aren't designed for filtering, especially since numbers are all treated with strong similarity.\n",
    "\n",
    "Let's improve the search by requesting that the AI return some specific price filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6c6255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup code\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import utils\n",
    "from openai import OpenAI\n",
    "\n",
    "# If you change the environment variables, you need to restart the kernel\n",
    "base_url = utils.get_base_url()\n",
    "api_key = utils.get_api_key()\n",
    "\n",
    "if utils.MODE == \"github\":\n",
    "    model = \"openai/gpt-4.1-nano\"  # A fast, small model\n",
    "elif utils.MODE == \"ollama\":\n",
    "    model = \"llama3.1\"  # llama and ollama are not related. It's a coincidence\n",
    "\n",
    "# OpenAI client is a class. The old API used to use globals. Sometimes you might see code snippets for the old API. \n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=base_url,\n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498f7ae0",
   "metadata": {},
   "source": [
    "# Defining tools\n",
    "\n",
    "As with most things, OpenAI did this first and most other models copied. So even if you're not using OpenAI models, see [OpenAI Spec](https://platform.openai.com/docs/guides/function-calling?api-mode=responses#defining-functions) for usage.\n",
    "\n",
    "Tools have:\n",
    " - A type (e.g. `function`)\n",
    " - A name which should be snake-case\n",
    " - A description. This is important, especially if there are multiple tools. Treat it like a prompt.\n",
    " - Parameters\n",
    " - Whether the function is \"strict\" i.e. function calls reliably adhere to the function schema, instead of being best effort. We recommend always enabling strict mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518dd3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tool for filtering prices within a range. Is not required. User could say \"less than $10\", or \"between $5 and $10\".\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"price_filter\",\n",
    "            \"description\": \"Filter prices within a range.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"min_price\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"Minimum price to filter.\",\n",
    "                    },\n",
    "                    \"max_price\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"Maximum price to filter.\",\n",
    "                    },\n",
    "                },\n",
    "                # Both parameters are optional. But this is how you could specify them as required.\n",
    "                # \"required\": [\"min_price\", \"max_price\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb09698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try that with a prompt\n",
    "\n",
    "def query_with_filter(query):\n",
    "    response =client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that can find products.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query,\n",
    "            }\n",
    "        ],\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",\n",
    "    )\n",
    "\n",
    "\n",
    "    if response.choices[0].message.tool_calls:\n",
    "        tool_call = response.choices[0].message.tool_calls[0]\n",
    "        if tool_call.function.name == \"price_filter\":\n",
    "            return tool_call.function.arguments\n",
    "\n",
    "print(query_with_filter(\"Find me a product that costs between $5 and $10.\"))\n",
    "print(query_with_filter(\"I'm looking for a hat that's less than $10.\"))\n",
    "print(query_with_filter(\"Birthday scarf. At least $24.99.\"))\n",
    "print(query_with_filter(\"Mothers day gift ideas for 20-30 bucks.\"))\n",
    "print(query_with_filter(\"Find me a product that costs loads-a-money.\"))  # This one is a big ambiguous, but lets see what it does"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8469e7e9",
   "metadata": {},
   "source": [
    "## Task: \n",
    "\n",
    "1. Experiment with some different questions and see what the filters look like.\n",
    "1. Modify the function below to filter out the dataframe by price before sorting it by similarity\n",
    "1. Update the `rag_chat` function and complete the two TODO items\n",
    "1. Test the discussion with various searches. Start with the suggestions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acd19e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the tool call responses\n",
    "from typing import Optional\n",
    "import utils\n",
    "from utils.embeddings import get_embedding_client, cosine_similarity, get_embedding, load_clothing_data  # See utils/embeddings.py for the cosine similarity function (its not complicated)\n",
    "\n",
    "embedding_client, dimensions, embedding_model = get_embedding_client()\n",
    "\n",
    "def search_df(df, product_description, n=3, min_price : Optional[float] = None, max_price: Optional[float] = None):\n",
    "    embedding = get_embedding(embedding_client, model=embedding_model, dimensions=dimensions, input=product_description)\n",
    "    df['similarities'] = df.embedding.apply(lambda x: cosine_similarity(x, embedding))\n",
    "    # TODO : filter the dataframe by price\n",
    "\n",
    "    res = df.sort_values('similarities', ascending=False).head(n)\n",
    "    return res\n",
    "\n",
    "data = load_clothing_data(embedding_model)\n",
    "\n",
    "\n",
    "def rag_chat(query, n=3):\n",
    "    # TODO: Get the filter parameters for `query`\n",
    "\n",
    "    # TODO: Update the search function to filter by price\n",
    "    matches = search_df(data, query, n=n)\n",
    "    \n",
    "    # Merge this into a prompt\n",
    "    # TODO : Find your \"winning prompt from the last exercise\"\n",
    "    system_prompt = f\"\"\"\n",
    "    The user has asked about a product, you are a helpful assistant that can give suggestions about products we have. \n",
    "\n",
    "    The matching products are:\n",
    "    \"\"\"\n",
    "\n",
    "    for match in matches.iterrows():\n",
    "        match = match[1]\n",
    "        system_prompt += f\"\"\"\n",
    "        Name: {match['name']}\n",
    "        Description: {match.description}\n",
    "        URL: https://www.superpythonshop.com/products/{match.id}\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    # Step 2: Call the model with the prompt\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "        ],\n",
    "        temperature=0.5,\n",
    "        n=1,\n",
    "    )\n",
    "\n",
    "    # Step 3: Return the response\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(rag_chat(\"I need a warm hat for winter less than $20\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada6a0ac",
   "metadata": {},
   "source": [
    "# Elemental Clash AI ✨✨✨\n",
    "\n",
    "I've taken 7 photos of playing cards on a table and we're going to get the AI to work out which player wins in our game **Elemental Clash**.\n",
    "\n",
    "![Card picture](data/cards/IMG_9059.jpg)\n",
    "\n",
    "If you want to add your own photos using a phone, please do!\n",
    "\n",
    "First, lets define the data structure for the cards so we can use Structured Outputs to get the cards played as PyDantic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b82835f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PlayedCards(cards=[ace of ♠, queen of ♥])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from enum import StrEnum # Python 3.11 + otherwise use (str, Enum) as base class\n",
    "from enum import Enum\n",
    "from pydantic import BaseModel\n",
    "from typing import cast\n",
    "\n",
    "\n",
    "class CardSuit(str, Enum):\n",
    "    hearts = \"hearts\"\n",
    "    diamonds = \"diamonds\"\n",
    "    clubs = \"clubs\"\n",
    "    spades = \"spades\"\n",
    "\n",
    "\n",
    "class CardValue(str, Enum):\n",
    "    two = \"2\"\n",
    "    three = \"3\"\n",
    "    four = \"4\"\n",
    "    five = \"5\"\n",
    "    six = \"6\"\n",
    "    seven = \"7\"\n",
    "    eight = \"8\"\n",
    "    nine = \"9\"\n",
    "    ten = \"10\"\n",
    "    jack = \"J\"\n",
    "    queen = \"Q\"\n",
    "    king = \"K\"\n",
    "    ace = \"A\"\n",
    "\n",
    "SUIT_GLYPHS = {\n",
    "    CardSuit.hearts: \"♥\",\n",
    "    CardSuit.diamonds: \"♦\",\n",
    "    CardSuit.clubs: \"♣\",\n",
    "    CardSuit.spades: \"♠\",\n",
    "}\n",
    "\n",
    "class PlayedCard(BaseModel):\n",
    "    suit: CardSuit\n",
    "    value: CardValue\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.value.name} of {SUIT_GLYPHS[self.suit]}\"\n",
    "\n",
    "\n",
    "class PlayedCards(BaseModel):\n",
    "    cards: list[PlayedCard]\n",
    "\n",
    "\n",
    "def parse_played_cards(message) -> PlayedCards:\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"What cards were played.\"},\n",
    "            {\"role\": \"user\", \"content\": message},\n",
    "        ],\n",
    "        response_format=PlayedCards,\n",
    "    )\n",
    "\n",
    "\n",
    "    message = completion.choices[0].message\n",
    "    if message.refusal:\n",
    "        print(message.refusal)\n",
    "        raise ValueError(\"Could not parse the message.\")\n",
    "    else:\n",
    "        return cast(PlayedCards, message.parsed)\n",
    "\n",
    "parse_played_cards(\"John played the ace of spades. Sarita played the queen of hearts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587a208b",
   "metadata": {},
   "source": [
    "# Task\n",
    "\n",
    "Next, let's combine this with a function called `determine_player_programmatically` which will evaluate (using Python, not AI) which player wins a turn.\n",
    "\n",
    "You will need to:\n",
    "\n",
    "1. Add player's name to the structured output by adding it as an attribute to `PlayedCard`\n",
    "1. Verify it works\n",
    "1. Call `determine_winner_programmatically` with a dictionary where `key` is player name and `value` is the card.\n",
    "1. The value needs to be in the format of `[value] of [suit]` e.g. `\"2 of spades\"` or `\"K of hearts\"`\n",
    "1. Print the winning player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75219b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.game import determine_winner_programmatically\n",
    "\n",
    "sample_query = \"John played the ace of spades. Sarita played the queen of hearts.\"\n",
    "\n",
    "# TODO: implement as per instructions\n",
    "winner = ...\n",
    "\n",
    "assert winner == \"John\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931d255c",
   "metadata": {},
   "source": [
    "# Adding a visual model\n",
    "\n",
    "To recognise what cards are played on the table, we can use a visual, or \"multi-modal\" model.\n",
    "\n",
    "The GPT-4o series is one of the most popular multi-modal models and it also supports structured outputs.\n",
    "\n",
    "For local development, `gemma3`, `phi4` and `llama4:scout` are the best options (this week!).\n",
    "\n",
    "Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dde678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "image_model = \"openai/gpt-4.1-mini\" if utils.MODE == \"github\" else \"gemma3:4b\"\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "# If we're using Github, use my copy of the photo on github\n",
    "if utils.MODE == \"github\":\n",
    "    image_url = \"https://raw.githubusercontent.com/tonybaloney/PyCon-AI-Crash-Course/refs/heads/main/data/cards/IMG_9059.jpg\"\n",
    "else:\n",
    "    # If local, you can use that or you can base64 encode it\n",
    "    base64_image = encode_image(\"data/cards/IMG_9059.jpg\")\n",
    "    image_url = f\"data:image/jpeg;base64,{base64_image}\"\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=image_model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                { \"type\": \"text\", \n",
    "                   \"text\": \"What cards are on the table\" },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": image_url}\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "result = response.choices[0].message.content\n",
    "display(Markdown(result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e4e016",
   "metadata": {},
   "source": [
    "# Task\n",
    "\n",
    "1. Use the `parse_played_cards` or the structured output parameter to this code and see if you can get a list of cards as a Python object.\n",
    "1. Write a program to determine the winner from any photo in `data/cards` by combining everything you've learned in this module. Since we don't know the player name in the image, just assign them a number and say what their card was.\n",
    "\n",
    "NB: The smaller, local models might not be able to accurately get _all_ of the cards on the table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25307734",
   "metadata": {},
   "source": [
    "# Agentic Frameworks\n",
    "\n",
    "Everything you've done so far in this tutorial has led up to \"agents\"\n",
    "\n",
    "Agentic programming is a way of connecting tasks, whether they be AI-driven, programmatic, or human-driven.\n",
    "\n",
    "For our card game, the flow is:\n",
    "\n",
    "- Someone deals each player 5 cards\n",
    "- The players choose a card and place it face down\n",
    "- The cards are turned over\n",
    "- The cards are analysed (we just automated that part)\n",
    "- The winner is decided\n",
    "\n",
    "Lets build a deck of cards in Python using a generator to deal from a deck. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08a1bc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1 played 10 of clubs\n",
      "Player 2 played Q of hearts\n",
      "Player 3 played 7 of diamonds\n",
      "Winner: Player 3\n",
      "\n",
      "Player 1 played J of clubs\n",
      "Player 2 played K of clubs\n",
      "Player 3 played 5 of hearts\n",
      "Winner: Player 2\n",
      "\n",
      "Player 1 played J of diamonds\n",
      "Player 2 played 10 of hearts\n",
      "Player 3 played 7 of spades\n",
      "Winner: Player 3\n",
      "\n",
      "Player 1 played A of hearts\n",
      "Player 2 played A of clubs\n",
      "Player 3 played 3 of hearts\n",
      "Winner: Player 2\n",
      "\n",
      "Player 1 played 10 of diamonds\n",
      "Player 2 played 9 of diamonds\n",
      "Player 3 played 7 of clubs\n",
      "Winner: Player 3\n",
      "\n",
      "Final Scores:\n",
      "Player 1: 0 points\n",
      "Player 2: 2 points\n",
      "Player 3: 3 points\n"
     ]
    }
   ],
   "source": [
    "from utils.game import deal, determine_winner_programmatically\n",
    "\n",
    "deck = deal()  # Generates next card\n",
    "\n",
    "player1_hand = [next(deck) for _ in range(5)]\n",
    "player2_hand = [next(deck) for _ in range(5)]\n",
    "player3_hand = [next(deck) for _ in range(5)]\n",
    "\n",
    "scores = {\n",
    "    \"Player 1\": 0,\n",
    "    \"Player 2\": 0,\n",
    "    \"Player 3\": 0,\n",
    "}\n",
    "\n",
    "for turn in range(5):\n",
    "    print(\"Player 1 played\", player1_hand[turn])\n",
    "    print(\"Player 2 played\", player2_hand[turn])\n",
    "    print(\"Player 3 played\", player3_hand[turn])\n",
    "\n",
    "    # Determine winner\n",
    "    winner = determine_winner_programmatically(\n",
    "        {\n",
    "            \"Player 1\": player1_hand[turn], \n",
    "            \"Player 2\": player2_hand[turn], \n",
    "            \"Player 3\": player3_hand[turn]\n",
    "        }\n",
    "    )\n",
    "    print(f\"Winner: {winner}\\n\")\n",
    "    # Update scores\n",
    "    scores[winner] += 1\n",
    "\n",
    "print(\"Final Scores:\")\n",
    "for player, score in scores.items():\n",
    "    print(f\"{player}: {score} points\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "467a17e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'seven'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[33;03m    Determine the winner of a card game based on the played cards.\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m determine_winner_programmatically(\n\u001b[32m     28\u001b[39m         {\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlayer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m: \u001b[38;5;28mrepr\u001b[39m(card) \u001b[38;5;28;01mfor\u001b[39;00m i, card \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(cards.cards, start=\u001b[32m1\u001b[39m)}\n\u001b[32m     29\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m cardplaying_agent.run(\u001b[33m'\u001b[39m\u001b[33mWho wins this turn\u001b[39m\u001b[33m'\u001b[39m, deps=PlayedCards(cards=[\n\u001b[32m     33\u001b[39m     PlayedCard(suit=CardSuit.spades, value=CardValue.ace),\n\u001b[32m     34\u001b[39m     PlayedCard(suit=CardSuit.hearts, value=CardValue.queen),\n\u001b[32m     35\u001b[39m     PlayedCard(suit=CardSuit.clubs, value=CardValue.jack),\n\u001b[32m     36\u001b[39m ]))\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(result.output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/PyCon-AI-Crash-Course/.venv/lib/python3.12/site-packages/pydantic_ai/agent.py:451\u001b[39m, in \u001b[36mAgent.run\u001b[39m\u001b[34m(self, user_prompt, output_type, message_history, model, deps, model_settings, usage_limits, usage, infer_name, **_deprecated_kwargs)\u001b[39m\n\u001b[32m    439\u001b[39m     output_type = _deprecated_kwargs[\u001b[33m'\u001b[39m\u001b[33mresult_type\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter(\n\u001b[32m    442\u001b[39m     user_prompt=user_prompt,\n\u001b[32m    443\u001b[39m     output_type=output_type,\n\u001b[32m   (...)\u001b[39m\u001b[32m    449\u001b[39m     usage=usage,\n\u001b[32m    450\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m agent_run:\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m agent_run:\n\u001b[32m    452\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m agent_run.result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m'\u001b[39m\u001b[33mThe graph run did not finish properly\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/PyCon-AI-Crash-Course/.venv/lib/python3.12/site-packages/pydantic_ai/agent.py:1798\u001b[39m, in \u001b[36mAgentRun.__anext__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1794\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__anext__\u001b[39m(\n\u001b[32m   1795\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1796\u001b[39m ) -> _agent_graph.AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]:\n\u001b[32m   1797\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Advance to the next node automatically based on the last returned node.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1798\u001b[39m     next_node = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._graph_run.\u001b[34m__anext__\u001b[39m()\n\u001b[32m   1799\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _agent_graph.is_agent_node(next_node):\n\u001b[32m   1800\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m next_node\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/PyCon-AI-Crash-Course/.venv/lib/python3.12/site-packages/pydantic_graph/graph.py:810\u001b[39m, in \u001b[36mGraphRun.__anext__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    807\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._next_node, End):\n\u001b[32m    808\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m810\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.next(\u001b[38;5;28mself\u001b[39m._next_node)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/PyCon-AI-Crash-Course/.venv/lib/python3.12/site-packages/pydantic_graph/graph.py:783\u001b[39m, in \u001b[36mGraphRun.next\u001b[39m\u001b[34m(self, node)\u001b[39m\n\u001b[32m    781\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.persistence.record_run(node_snapshot_id):\n\u001b[32m    782\u001b[39m         ctx = GraphRunContext(\u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.deps)\n\u001b[32m--> \u001b[39m\u001b[32m783\u001b[39m         \u001b[38;5;28mself\u001b[39m._next_node = \u001b[38;5;28;01mawait\u001b[39;00m node.run(ctx)\n\u001b[32m    785\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._next_node, End):\n\u001b[32m    786\u001b[39m     \u001b[38;5;28mself\u001b[39m._snapshot_id = \u001b[38;5;28mself\u001b[39m._next_node.get_snapshot_id()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/PyCon-AI-Crash-Course/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py:382\u001b[39m, in \u001b[36mCallToolsNode.run\u001b[39m\u001b[34m(self, ctx)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\n\u001b[32m    380\u001b[39m     \u001b[38;5;28mself\u001b[39m, ctx: GraphRunContext[GraphAgentState, GraphAgentDeps[DepsT, NodeRunEndT]]\n\u001b[32m    381\u001b[39m ) -> Union[ModelRequestNode[DepsT, NodeRunEndT], End[result.FinalResult[NodeRunEndT]]]:  \u001b[38;5;66;03m# noqa UP007\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m382\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream(ctx):\n\u001b[32m    383\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._next_node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m'\u001b[39m\u001b[33mthe stream should set `self._next_node` before it ends\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:217\u001b[39m, in \u001b[36m_AsyncGeneratorContextManager.__aexit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m anext(\u001b[38;5;28mself\u001b[39m.gen)\n\u001b[32m    218\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n\u001b[32m    219\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/PyCon-AI-Crash-Course/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py:396\u001b[39m, in \u001b[36mCallToolsNode.stream\u001b[39m\u001b[34m(self, ctx)\u001b[39m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m stream\n\u001b[32m    395\u001b[39m \u001b[38;5;66;03m# Run the stream to completion if it was not finished:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m396\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _event \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[32m    397\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/PyCon-AI-Crash-Course/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py:445\u001b[39m, in \u001b[36mCallToolsNode._run_stream\u001b[39m\u001b[34m(self, ctx)\u001b[39m\n\u001b[32m    441\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m exceptions.UnexpectedModelBehavior(\u001b[33m'\u001b[39m\u001b[33mReceived empty model response\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    443\u001b[39m     \u001b[38;5;28mself\u001b[39m._events_iterator = _run_stream()\n\u001b[32m--> \u001b[39m\u001b[32m445\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._events_iterator:\n\u001b[32m    446\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m event\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/PyCon-AI-Crash-Course/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py:423\u001b[39m, in \u001b[36mCallToolsNode._run_stream.<locals>._run_stream\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    418\u001b[39m \u001b[38;5;66;03m# At the moment, we prioritize at least executing tool calls if they are present.\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[38;5;66;03m# In the future, we'd consider making this configurable at the agent or run level.\u001b[39;00m\n\u001b[32m    420\u001b[39m \u001b[38;5;66;03m# This accounts for cases like anthropic returns that might contain a text response\u001b[39;00m\n\u001b[32m    421\u001b[39m \u001b[38;5;66;03m# and a tool call response, where the text response just indicates the tool call will happen.\u001b[39;00m\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tool_calls:\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._handle_tool_calls(ctx, tool_calls):\n\u001b[32m    424\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m event\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m texts:\n\u001b[32m    426\u001b[39m     \u001b[38;5;66;03m# No events are emitted during the handling of text responses, so we don't need to yield anything\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/PyCon-AI-Crash-Course/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py:474\u001b[39m, in \u001b[36mCallToolsNode._handle_tool_calls\u001b[39m\u001b[34m(self, ctx, tool_calls)\u001b[39m\n\u001b[32m    472\u001b[39m \u001b[38;5;66;03m# Then build the other request parts based on end strategy\u001b[39;00m\n\u001b[32m    473\u001b[39m tool_responses: \u001b[38;5;28mlist\u001b[39m[_messages.ModelRequestPart] = \u001b[38;5;28mself\u001b[39m._tool_responses\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m process_function_tools(\n\u001b[32m    475\u001b[39m     tool_calls,\n\u001b[32m    476\u001b[39m     final_result \u001b[38;5;129;01mand\u001b[39;00m final_result.tool_name,\n\u001b[32m    477\u001b[39m     final_result \u001b[38;5;129;01mand\u001b[39;00m final_result.tool_call_id,\n\u001b[32m    478\u001b[39m     ctx,\n\u001b[32m    479\u001b[39m     tool_responses,\n\u001b[32m    480\u001b[39m ):\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m event\n\u001b[32m    483\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m final_result:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/PyCon-AI-Crash-Course/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py:665\u001b[39m, in \u001b[36mprocess_function_tools\u001b[39m\u001b[34m(tool_calls, output_tool_name, output_tool_call_id, ctx, output_parts)\u001b[39m\n\u001b[32m    663\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m done:\n\u001b[32m    664\u001b[39m     index = tasks.index(task)\n\u001b[32m--> \u001b[39m\u001b[32m665\u001b[39m     result = \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    666\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m _messages.FunctionToolResultEvent(result, tool_call_id=call_index_to_event_id[index])\n\u001b[32m    668\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, _messages.RetryPromptPart):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/PyCon-AI-Crash-Course/.venv/lib/python3.12/site-packages/pydantic_ai/tools.py:329\u001b[39m, in \u001b[36mTool.run\u001b[39m\u001b[34m(self, message, run_context, tracer)\u001b[39m\n\u001b[32m    310\u001b[39m span_attributes = {\n\u001b[32m    311\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mgen_ai.tool.name\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.name,\n\u001b[32m    312\u001b[39m     \u001b[38;5;66;03m# NOTE: this means `gen_ai.tool.call.id` will be included even if it was generated by pydantic-ai\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    326\u001b[39m     ),\n\u001b[32m    327\u001b[39m }\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tracer.start_as_current_span(\u001b[33m'\u001b[39m\u001b[33mrunning tool\u001b[39m\u001b[33m'\u001b[39m, attributes=span_attributes):\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run(message, run_context)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/PyCon-AI-Crash-Course/.venv/lib/python3.12/site-packages/pydantic_ai/tools.py:346\u001b[39m, in \u001b[36mTool._run\u001b[39m\u001b[34m(self, message, run_context)\u001b[39m\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_async:\n\u001b[32m    345\u001b[39m     function = cast(Callable[[Any], Awaitable[\u001b[38;5;28mstr\u001b[39m]], \u001b[38;5;28mself\u001b[39m.function)\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     response_content = \u001b[38;5;28;01mawait\u001b[39;00m function(*args, **kwargs)\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    348\u001b[39m     function = cast(Callable[[Any], \u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mself\u001b[39m.function)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mdetermine_winner\u001b[39m\u001b[34m(context, cards)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;129m@cardplaying_agent\u001b[39m.tool\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdetermine_winner\u001b[39m(\n\u001b[32m     21\u001b[39m     context: RunContext,\n\u001b[32m     22\u001b[39m     cards: PlayedCards,\n\u001b[32m     23\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     24\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[33;03m    Determine the winner of a card game based on the played cards.\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdetermine_winner_programmatically\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPlayer \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcard\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcard\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcards\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/PyCon-AI-Crash-Course/utils/game.py:101\u001b[39m, in \u001b[36mdetermine_winner_programmatically\u001b[39m\u001b[34m(player_cards)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# Compare each player's card with the current winner\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(player_cards_sequence)):\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     winner_o = \u001b[43mwinner_of\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwinner\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayer_cards_sequence\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m winner_o == player_cards_sequence[i][\u001b[32m1\u001b[39m]:\n\u001b[32m    103\u001b[39m         winner = player_cards_sequence[i]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/PyCon-AI-Crash-Course/utils/game.py:80\u001b[39m, in \u001b[36mdetermine_winner_programmatically.<locals>.winner_of\u001b[39m\u001b[34m(player1, player2)\u001b[39m\n\u001b[32m     77\u001b[39m value1, _, suit1 = player1.split(\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     78\u001b[39m value2, _, suit2 = player2.split(\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m value1 = \u001b[43mSEQUENCE_VALUE\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalue1\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     81\u001b[39m value2 = SEQUENCE_VALUE[value2]\n\u001b[32m     83\u001b[39m suit1 = suit1.lower()\n",
      "\u001b[31mKeyError\u001b[39m: 'seven'"
     ]
    }
   ],
   "source": [
    "from pydantic_ai import Agent, RunContext\n",
    "from pydantic_ai.models.openai import OpenAIModel\n",
    "from pydantic_ai.providers.openai import OpenAIProvider\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "# Pydantic AI wants async clients by default\n",
    "pai_model = OpenAIModel(\n",
    "    model_name=model,\n",
    "    provider=OpenAIProvider(openai_client=AsyncOpenAI(base_url=base_url, api_key=api_key)),\n",
    ")\n",
    "\n",
    "cardplaying_agent = Agent(\n",
    "    pai_model,\n",
    "    deps_type=PlayedCards,\n",
    "    output_type=str,\n",
    "    system_prompt=\"You are a card game assistant. You determine the winner using determine_winner\",\n",
    ")\n",
    "\n",
    "@cardplaying_agent.tool\n",
    "async def determine_winner(\n",
    "    context: RunContext,\n",
    "    cards: PlayedCards,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Determine the winner of a card game based on the played cards.\n",
    "    \"\"\"\n",
    "    return determine_winner_programmatically(\n",
    "        {f\"Player {i}\": repr(card) for i, card in enumerate(cards.cards, start=1)}\n",
    "    )\n",
    "\n",
    "\n",
    "result = await cardplaying_agent.run('Who wins this turn', deps=PlayedCards(cards=[\n",
    "    PlayedCard(suit=CardSuit.spades, value=CardValue.ace),\n",
    "    PlayedCard(suit=CardSuit.hearts, value=CardValue.queen),\n",
    "    PlayedCard(suit=CardSuit.clubs, value=CardValue.jack),\n",
    "]))\n",
    "print(result.output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
