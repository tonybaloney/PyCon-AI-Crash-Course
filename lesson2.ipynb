{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96ba41d5",
   "metadata": {},
   "source": [
    "# Lesson 2: Embedding models\n",
    "\n",
    "In this exercise, we'll explore embedding models.\n",
    "\n",
    "Most embedding models handle text input and output floating point numbers. \n",
    "\n",
    "The **dimensions** is the number of dimensions in that model. The \"embeddings\", or \"vectors\" are the floating point numbers.\n",
    "\n",
    "Like the chat completions client, the OpenAI SDK has become the standard for how people use embedding models.\n",
    "\n",
    "For this exercise, I have pre-computed the embeddings for a \"database\" of 166 items of clothing for a shop (see data/clothes.json). This will save you time. \n",
    "\n",
    "For local development, you can either use `nomic-embed-text` (v1) which is English-only, or `granite-embedding:278m` which is multilingual. Nomic has fewer parameters so it requires less memory to calculate embeddings.\n",
    "\n",
    "```bash\n",
    "ollama pull nomic-embed-text\n",
    "ollama pull granite-embedding:278m\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fe48b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your first embedding with a model\n",
    "from openai import OpenAI\n",
    "import utils\n",
    "\n",
    "# If you change the environment variables, you need to restart the kernel\n",
    "api_key = utils.get_api_key()\n",
    "\n",
    "if utils.MODE == \"github\":\n",
    "    model = \"text-embedding-3-small\"  # A fast, small model\n",
    "    base_url = \"https://models.inference.ai.azure.com\"\n",
    "    # default is 1536 for text-embedding-3-small. Is not an arbitrary number, is one of the accepted values (256, 512, 1024, etc.)\n",
    "    # ada-002 doesn't support variable dimensions\n",
    "    dimensions = 1024 \n",
    "elif utils.MODE == \"ollama\":\n",
    "    # pick which one works for you\n",
    "    # model = \"nomic-embed-text\"  # A comparable open-source model\n",
    "    model = \"granite-embedding:278m\"  # a multilingual model\n",
    "    base_url = utils.get_base_url()\n",
    "    dimensions = 768  # Both granite and nomic-embed-text have 768 dimensions. If you provide a different number, it will ignore you and still return 768 dimensions.\n",
    "\n",
    "# OpenAI client is a class. The old API used to use globals. Sometimes you might see code snippets for the old API. \n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=base_url,\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "def get_embedding(text):\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=model,\n",
    "        dimensions=dimensions,\n",
    "    )\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d376fe14",
   "metadata": {},
   "source": [
    "First lets try getting an embedding for a piece of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46deef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "beans_embedding = get_embedding(\"delicious beans\")\n",
    "print(len(beans_embedding), beans_embedding[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c590f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.embeddings import cosine_similarity  # See utils/embeddings.py for the cosine similarity function (its not complicated)\n",
    "\n",
    "data = pd.read_json(\"data/clothes.json\")\n",
    "if utils.MODE == \"ollama\":\n",
    "    if model == \"nomic-embed-text\":\n",
    "        data['embedding'] = data.embedding_nomic\n",
    "    elif model == \"granite-embedding:278m\":\n",
    "        data['embedding'] = data.embedding_granite\n",
    "    else:\n",
    "        raise ValueError(\"I didn't precompute those embeddings. \")\n",
    "\n",
    "\n",
    "def search_df(df, product_description, n=3):\n",
    "    embedding = get_embedding(product_description)\n",
    "    df['similarities'] = df.embedding.apply(lambda x: cosine_similarity(x, embedding))\n",
    "    res = df.sort_values('similarities', ascending=False).head(n)\n",
    "    return res\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cd3d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = search_df(data, 'fishing gear', n=3)\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86625998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This works for many languages, not just English\n",
    "# Nb: the nomic-embed-text model is barely multilingual. Results will differ greatly with text-embedding-3-small\n",
    "search_df(data, 'Equipo de pesca', n=3)  # Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39572a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_df(data, '釣りの物' , n=3)  # Japanese"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6218868",
   "metadata": {},
   "source": [
    "Even though we searched for the same thing in 3 different languages, the similarity score (right column) was quite different.\n",
    "The embedding models are multilingual but same-language will score higher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abe9486",
   "metadata": {},
   "source": [
    "Computing the similarities for every item is highly intensive, so we can use indexes to cluster vectors together to speed up the search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef99c634",
   "metadata": {},
   "source": [
    "# Combining Text Completions and Embeddings to make a RAG bot\n",
    "\n",
    "We can combine the text-completions (LLM) with the embedding search to find relevant products and include them in the chat.\n",
    "\n",
    "This information could also be something like a knowledgebase, wiki, or an unstructured data store.\n",
    "\n",
    "The stages are:\n",
    "\n",
    "1. Get the request from the user\n",
    "1. Search the embedding index for similar matches\n",
    "1. Give those matches to the LLM along with the original question or query\n",
    "1. Ask it to generate a response\n",
    "1. Give the response back to the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970ea7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if utils.MODE == \"github\":\n",
    "    chat_model = \"gpt-4.1-nano\"  # A fast, small model\n",
    "elif utils.MODE == \"ollama\":\n",
    "    chat_model = \"llama3.1\"  # llama and ollama are not related. It's a coincidence\n",
    "\n",
    "\n",
    "def rag_chat(query, n=3):\n",
    "    # Step 1: Get the embedding for the query\n",
    "    matches = search_df(data, query, n=n)\n",
    "    \n",
    "    # Merge this into a prompt\n",
    "    system_prompt = \"\"\"\n",
    "    The user has asked about a product, you are a helpful assistant that can give suggestions about products we have. \n",
    "\n",
    "    The matching products are:\n",
    "    \"\"\"\n",
    "\n",
    "    for match in matches.iterrows():\n",
    "        match = match[1]\n",
    "        system_prompt += f\"\"\"\n",
    "        Name: {match['name']}\n",
    "        Description: {match.description}\n",
    "        URL: https://www.superpythonshop.com/products/{match.id}\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    # Step 2: Call the model with the prompt\n",
    "    response = client.chat.completions.create(\n",
    "        model=chat_model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "        ],\n",
    "        temperature=0.5,\n",
    "        n=1,\n",
    "    )\n",
    "\n",
    "    # Step 3: Return the response\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(rag_chat(\"I need a warm hat for winter\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117c836f",
   "metadata": {},
   "source": [
    "# Task\n",
    "\n",
    "Your next job is to iterate on this prompt to refine it and improve the suggestions. Try different queries and searches to see what it does.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "- Edit the cell above and change the system prompt\n",
    "- Run the cell again to see the results\n",
    "\n",
    "Try the following:\n",
    "\n",
    "- Looking for something silly\n",
    "- Looking for something that doesn't exist\n",
    "- Starting an argument with it\n",
    "- Asking a question with errors\n",
    "- Asking a question in a different language\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
