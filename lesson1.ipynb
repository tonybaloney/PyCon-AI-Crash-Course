{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04614a99",
   "metadata": {},
   "source": [
    "# Section 1 Exercises\n",
    "\n",
    "## Setup\n",
    "\n",
    "### Using this notebook with ollama: \n",
    "\n",
    "- Install ollama\n",
    "- Pull the `llama3.1` model by running `ollama pull llama3.1`\n",
    "- Create `.env` and write\n",
    "\n",
    "```ini\n",
    "MODE=ollama\n",
    "```\n",
    "\n",
    "- Restart the kernel\n",
    "\n",
    "### Using this notebook with GitHub Models: \n",
    "\n",
    "- Create `.env` and write\n",
    "\n",
    "```ini\n",
    "MODE=github\n",
    "```\n",
    "\n",
    "- Go to [Access Tokens](https://github.com/settings/personal-access-tokens)\n",
    "- Create new **Fine-tuned token**\n",
    "- Set timeout to be some time in the future (how long you plan on using it)\n",
    "- Expand Account Permissions\n",
    "- Find **Models** and change to read-only\n",
    "- Save\n",
    "- Copy the key\n",
    "- Add it to `.env` as `GITHUB_TOKEN=gha....`\n",
    "- Restart the kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25924928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28c3a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to install the required packages, I suggest doing this first. \n",
    "%pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7082d84",
   "metadata": {},
   "source": [
    "# OpenAI Byte-Pair-Encodings\n",
    "\n",
    "Many AI models use OpenAI's Byte-Pair-Encodings\n",
    "\n",
    "- `cl100k_base` (GPT3.5 to GPT4)\n",
    "- `o200k_base` (GPT-4o, o1, o3, GPT-4.1)\n",
    "\n",
    "The easiest and fastest way to use these encodings is with the `tiktoken` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e92190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization for OpenAI models\n",
    "\n",
    "from tiktoken import encoding_for_model, get_encoding\n",
    "\n",
    "# Load the encoding for a known OpenAI model\n",
    "encoding = encoding_for_model(\"gpt-3.5-turbo\")\n",
    "# or use a specific encoding\n",
    "# encoding = get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Encode the text\n",
    "text = \"Hello, world!\"\n",
    "tokens = encoding.encode(text)\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10894acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokens back to text\n",
    "decoded_text = encoding.decode(tokens)\n",
    "\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82d5ce7",
   "metadata": {},
   "source": [
    "# Hugging Face Tokenizers\n",
    "\n",
    "Many open models use custom tokenizers and byte-pair-encodings.\n",
    "\n",
    "HuggingFace Models have an API for storing and downloading tokenizers to process inputs locally.\n",
    "\n",
    "The `tokenizers` package on PyPi facilitates this, but the `transformers` package makes it even easier to download and use them.\n",
    "\n",
    "Use the `AutoTokenizer` class to download and install the tokenizer for a given model.\n",
    "\n",
    "You will need to create an account on hugging face and login using:\n",
    "\n",
    "```bash\n",
    "huggingface_hub login\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962dfcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using huggingface tokenizers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer for a specific model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\")\n",
    "\n",
    "# Encode the text\n",
    "tokens = tokenizer(text, \n",
    "                   add_special_tokens=False, # End of stream/end of text tokens\n",
    "                   return_offsets_mapping=False) # Return offsets for each token\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b204713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, you can use the tokenizer directly\n",
    "tokens = tokenizer.encode(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6e3d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the tokens back to text\n",
    "decoded_text = tokenizer.decode(tokens)\n",
    "\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd54d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input text doesn't need to be English or ASCII\n",
    "text = \"你好，世界！\"\n",
    "tokens = encoding.encode(text)\n",
    "decoded_text = encoding.decode(tokens)\n",
    "\n",
    "tokens, decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ceeff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Although, the number of tokens may be different\n",
    "# depending on the encoding used\n",
    "text = \"你好，世界！\"\n",
    "\n",
    "encoding_gpt2 = get_encoding(\"gpt2\") # Old, but used in some open models\n",
    "encoding_cl100 = get_encoding(\"cl100k_base\") # GPT3.5 - 4\n",
    "encoding_o200 = get_encoding(\"o200k_base\") # New encoding. Double the size of cl100k_base and optimized for languages like Chinese and Japanese\n",
    "\n",
    "print(\"GPT-2 is \", len(encoding_gpt2.encode(text)), \"tokens\")\n",
    "print(\"CL100k_base is \", len(encoding_cl100.encode(text)), \"tokens\")\n",
    "print(\"and o200 is\", len(encoding_o200.encode(text)), \"tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642b4bb1",
   "metadata": {},
   "source": [
    "# Using OpenAI client for chat completions\n",
    "\n",
    "The OpenAI Python client has become the unofficial standard and most models are work with OpenAI's API spec.\n",
    "\n",
    "So, even if you're not using OpenAI you can still use the Python client to talk to GitHub models, Ollama local models and many 3rd party LLMs.\n",
    "\n",
    "Normally, you install it with `pip install openai` but it's already a requirement for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af19a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your first conversation with a model\n",
    "from openai import OpenAI\n",
    "import utils\n",
    "\n",
    "# If you change the environment variables, you need to restart the kernel\n",
    "base_url = utils.get_base_url()\n",
    "api_key = utils.get_api_key()\n",
    "\n",
    "if utils.MODE == \"github\":\n",
    "    model = \"openai/gpt-4.1-nano\"  # A fast, small model\n",
    "elif utils.MODE == \"ollama\":\n",
    "    model = \"llama3.1\"  # llama and ollama are not related. It's a coincidence\n",
    "\n",
    "# OpenAI client is a class. The old API used to use globals. Sometimes you might see code snippets for the old API. \n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=base_url,\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of France?\",\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.5,  # or top_p=0.9\n",
    "    n=1, # Number of results to return. If you want multiple options, increase this\n",
    "\n",
    "    # Here are some extra parameters you might need in future\n",
    "\n",
    "    # presence_penalty=0.0, # Increase the likelihood of new topics, default is 0. Range is -1 to 1\n",
    "    # frequency_penalty=0.0, # Increase the likelihood of new words, default is 0. Range is -2 to 2\n",
    "    # max_tokens=100, # Maximum number of tokens to return\n",
    "    # stop=None, # Stop when the model generates this token\n",
    ")\n",
    "\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e98d9d",
   "metadata": {},
   "source": [
    "# Exercise 1: Adjust the temperature and see how it affects the output\n",
    "\n",
    "Copy the code above and ask it to create a **poem about the moon**\n",
    "Try different values for temperature and see how it affects the output\n",
    "Try varying frequency_penalty and presence_penalty too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b84185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your moon poem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d837ae91",
   "metadata": {},
   "source": [
    "Below we have written a grading system using another LLM. You will get a grade from A-F along with the reasoning. See if you can adjust your prompt to get a better grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa64e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put a better poem here\n",
    "poem = \"\"\"\n",
    "Oh moonie moonie, shining bright,\n",
    "I love to see you in the night.\n",
    "You light the way for all to see,\n",
    "And fill my heart with joy and glee.\n",
    "\"\"\"\n",
    "\n",
    "grade = utils.grade_poem(client, model, poem)\n",
    "\n",
    "\n",
    "display(Markdown(grade))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451f4b07",
   "metadata": {},
   "source": [
    "# Zero, one and few shot prompts\n",
    "\n",
    "## Game\n",
    "\n",
    "We're going to test an AI by giving it a made-up card game and getting it to work out who the winner is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fa62e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.game import RULES\n",
    "\n",
    "display(Markdown(RULES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c625d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero, one, and few-shot prompting\n",
    "from utils.game import determine_winner_programmatically\n",
    "\n",
    "\n",
    "instructions = f\"\"\"You are an AI for a fictional card game. Given the players and the cards they play, you need to determine the winner of the game. Return only the number of the player.\n",
    "The rules of the game are:\n",
    "\n",
    "{RULES}\n",
    "\"\"\"\n",
    "\n",
    "def determine_winner(turn):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": instructions,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": turn,\n",
    "            }\n",
    "        ],\n",
    "        temperature=1.0,\n",
    "        n=1, # Number of results to return. If you want multiple options, increase this\n",
    "    )\n",
    "\n",
    "    result = response.choices[0].message.content\n",
    "    return result\n",
    "\n",
    "# Example turn\n",
    "test_turn = {\n",
    "    \"Player 1\": \"2 of hearts\",\n",
    "    \"Player 2\": \"3 of diamonds\",\n",
    "    \"Player 3\": \"4 of diamonds\"\n",
    "}\n",
    "\n",
    "turn = f\"\"\"\n",
    "Player 1: {test_turn['Player 1']}\n",
    "Player 2: {test_turn['Player 2']}\n",
    "Player 3: {test_turn['Player 3']}\n",
    "\"\"\"\n",
    "ai_result = determine_winner(turn)\n",
    "actual_winner = determine_winner_programmatically(test_turn)\n",
    "\n",
    "if ai_result == actual_winner:\n",
    "    display(Markdown(f\"AI result: {ai_result} is correct!\"))\n",
    "else:\n",
    "    display(Markdown(f\"AI result: {ai_result} is incorrect! Actual result: {actual_winner}\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a3ba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Introduce one-shot by extending the `instructions` and including an example turn and result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b85865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Introduce few-shot by extending the messages with example turns and results\n",
    "\n",
    "\"\"\"\n",
    "e.g. \n",
    "\n",
    "messages=[\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": instructions,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Player 1: 2 of hearts\\nPlayer 2: 3 of spades\\nPlayer 3: 4 of diamonds\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": f\"Player 2 wins!\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": turn,\n",
    "    },\n",
    "\n",
    "]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ae6f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, test out the AI with a few turns\n",
    "from utils.game import create_training_data\n",
    "\n",
    "total_score = 0\n",
    "\n",
    "for plays, winner in create_training_data(10):\n",
    "    turn = f\"\"\"\n",
    "    Player 1: {test_turn['Player 1']}\n",
    "    Player 2: {test_turn['Player 2']}\n",
    "    Player 3: {test_turn['Player 3']}\n",
    "    \"\"\"\n",
    "    ai_result = determine_winner(turn)\n",
    "    actual_winner = winner\n",
    "\n",
    "    if ai_result == actual_winner:\n",
    "        display(Markdown(f\"AI result: {ai_result} is correct!\"))\n",
    "        total_score += 1\n",
    "    else:\n",
    "        display(Markdown(f\"AI result: {ai_result} is incorrect! Actual result: {actual_winner}\"))\n",
    "\n",
    "display(Markdown(f\"Your score is {total_score} out of 10\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2320971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus task.\n",
    "\n",
    "# If you finish early, try and experiment with what happens when you change the number of players."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
