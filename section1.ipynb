{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04614a99",
   "metadata": {},
   "source": [
    "# Section 1 Exercises\n",
    "\n",
    "## Setup\n",
    "\n",
    "### Using this notebook with ollama: \n",
    "\n",
    "- Install ollama\n",
    "- Pull the `llama3.1` model by running `ollama pull llama3.1`\n",
    "- Create `.env` and write\n",
    "\n",
    "```ini\n",
    "MODE=ollama\n",
    "```\n",
    "\n",
    "- Restart the kernel\n",
    "\n",
    "### Using this notebook with GitHub Models: \n",
    "\n",
    "- Create `.env` and write\n",
    "\n",
    "```ini\n",
    "MODE=github\n",
    "```\n",
    "\n",
    "- Go to [Access Tokens](https://github.com/settings/personal-access-tokens)\n",
    "- Create new **Fine-tuned token**\n",
    "- Set timeout to be some time in the future (how long you plan on using it)\n",
    "- Expand Account Permissions\n",
    "- Find **Models** and change to read-only\n",
    "- Save\n",
    "- Copy the key\n",
    "- Add it to `.env` as `GITHUB_TOKEN=gha....`\n",
    "- Restart the kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b28c3a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (0.9.0)\n",
      "Requirement already satisfied: openai in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (1.77.0)\n",
      "Requirement already satisfied: tokenizers in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (0.21.1)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (4.51.3)\n",
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (1.1.0)\n",
      "Requirement already satisfied: huggingface_hub[xet] in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (0.31.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.12/site-packages (from tiktoken->-r requirements.txt (line 1)) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./.venv/lib/python3.12/site-packages (from tiktoken->-r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.12/site-packages (from openai->-r requirements.txt (line 2)) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.12/site-packages (from openai->-r requirements.txt (line 2)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.12/site-packages (from openai->-r requirements.txt (line 2)) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.12/site-packages (from openai->-r requirements.txt (line 2)) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.venv/lib/python3.12/site-packages (from openai->-r requirements.txt (line 2)) (2.11.4)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.12/site-packages (from openai->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.12/site-packages (from openai->-r requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./.venv/lib/python3.12/site-packages (from openai->-r requirements.txt (line 2)) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 2)) (3.10)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 2)) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 2)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 2)) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 2)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 2)) (0.4.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from huggingface_hub[xet]->-r requirements.txt (line 5)) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface_hub[xet]->-r requirements.txt (line 5)) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.12/site-packages (from huggingface_hub[xet]->-r requirements.txt (line 5)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from huggingface_hub[xet]->-r requirements.txt (line 5)) (6.0.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from huggingface_hub[xet]->-r requirements.txt (line 5)) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 4)) (2.2.5)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 4)) (0.5.3)\n",
      "\u001b[33mWARNING: huggingface-hub 0.31.1 does not provide the extra 'xet'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken->-r requirements.txt (line 1)) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken->-r requirements.txt (line 1)) (2.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7082d84",
   "metadata": {},
   "source": [
    "# OpenAI Byte-Pair-Encodings\n",
    "\n",
    "Many AI models use OpenAI's Byte-Pair-Encodings\n",
    "\n",
    "- `cl100k_base` (GPT3.5 to GPT4)\n",
    "- `o200k_base` (GPT-4o, o1, o3, GPT-4.1)\n",
    "\n",
    "The easiest and fastest way to use these encodings is with the `tiktoken` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03e92190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9906, 11, 1917, 0]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization for OpenAI models\n",
    "\n",
    "from tiktoken import encoding_for_model, get_encoding\n",
    "\n",
    "# Load the encoding for a known OpenAI model\n",
    "encoding = encoding_for_model(\"gpt-3.5-turbo\")\n",
    "# or use a specific encoding\n",
    "# encoding = get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Encode the text\n",
    "text = \"Hello, world!\"\n",
    "tokens = encoding.encode(text)\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10894acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, world!'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert tokens back to text\n",
    "decoded_text = encoding.decode(tokens)\n",
    "\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82d5ce7",
   "metadata": {},
   "source": [
    "# Hugging Face Tokenizers\n",
    "\n",
    "Many open models use custom tokenizers and byte-pair-encodings.\n",
    "\n",
    "HuggingFace Models have an API for storing and downloading tokenizers to process inputs locally.\n",
    "\n",
    "The `tokenizers` package on PyPi facilitates this, but the `transformers` package makes it even easier to download and use them.\n",
    "\n",
    "Use the `AutoTokenizer` class to download and install the tokenizer for a given model.\n",
    "\n",
    "You will need to create an account on hugging face and login using:\n",
    "\n",
    "```bash\n",
    "huggingface_hub login\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "962dfcb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonyshaw/repos/PyCon-AI-Crash-Course/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [9707, 11, 1879, 0], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using huggingface tokenizers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer for a specific model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\")\n",
    "\n",
    "# Encode the text\n",
    "tokens = tokenizer(text, \n",
    "                   add_special_tokens=False, # End of stream/end of text tokens\n",
    "                   return_offsets_mapping=False) # Return offsets for each token\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b204713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9707, 11, 1879, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternatively, you can use the tokenizer directly\n",
    "tokens = tokenizer.encode(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd6e3d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, world!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode the tokens back to text\n",
    "decoded_text = tokenizer.decode(tokens)\n",
    "\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cd54d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([57668, 53901, 3922, 3574, 244, 98220, 6447], '你好，世界！')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input text doesn't need to be English or ASCII\n",
    "text = \"你好，世界！\"\n",
    "tokens = encoding.encode(text)\n",
    "decoded_text = encoding.decode(tokens)\n",
    "\n",
    "tokens, decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52ceeff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 is  14 tokens\n",
      "CL100k_base is  7 tokens\n",
      "and o200 is 4 tokens\n"
     ]
    }
   ],
   "source": [
    "# Although, the number of tokens may be different\n",
    "# depending on the encoding used\n",
    "text = \"你好，世界！\"\n",
    "\n",
    "encoding_gpt2 = get_encoding(\"gpt2\") # Old, but used in some open models\n",
    "encoding_cl100 = get_encoding(\"cl100k_base\") # GPT3.5 - 4\n",
    "encoding_o200 = get_encoding(\"o200k_base\") # New encoding. Double the size of cl100k_base and optimized for languages like Chinese and Japanese\n",
    "\n",
    "print(\"GPT-2 is \", len(encoding_gpt2.encode(text)), \"tokens\")\n",
    "print(\"CL100k_base is \", len(encoding_cl100.encode(text)), \"tokens\")\n",
    "print(\"and o200 is\", len(encoding_o200.encode(text)), \"tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642b4bb1",
   "metadata": {},
   "source": [
    "# Using OpenAI client for chat completions\n",
    "\n",
    "The OpenAI Python client has become the unofficial standard and most models are work with OpenAI's API spec.\n",
    "\n",
    "So, even if you're not using OpenAI you can still use the Python client to talk to GitHub models, Ollama local models and many 3rd party LLMs.\n",
    "\n",
    "Normally, you install it with `pip install openai` but it's already a requirement for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2af19a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your first conversation with a model\n",
    "from openai import OpenAI\n",
    "import utils\n",
    "\n",
    "# If you change the environment variables, you need to restart the kernel\n",
    "base_url = utils.get_base_url()\n",
    "api_key = utils.get_api_key()\n",
    "\n",
    "if utils.MODE == \"github\":\n",
    "    model = \"openai/gpt-4.1-nano\"  # A fast, small model\n",
    "elif utils.MODE == \"ollama\":\n",
    "    model = \"llama3.1\"  # llama and ollama are not related. It's a coincidence\n",
    "\n",
    "# OpenAI client is a class. The old API used to use globals. Sometimes you might see code snippets for the old API. \n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=base_url,\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the capital of France?\",\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.5,  # or top_p=0.9\n",
    "    n=1, # Number of results to return. If you want multiple options, increase this\n",
    "\n",
    "    # Here are some extra parameters you might need in future\n",
    "\n",
    "    # presence_penalty=0.0, # Increase the likelihood of new topics, default is 0. Range is -1 to 1\n",
    "    # frequency_penalty=0.0, # Increase the likelihood of new words, default is 0. Range is -2 to 2\n",
    "    # max_tokens=100, # Maximum number of tokens to return\n",
    "    # stop=None, # Stop when the model generates this token\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e98d9d",
   "metadata": {},
   "source": [
    "# Exercise 1: Adjust the temperature and see how it affects the output\n",
    "\n",
    "Copy the code above and ask it to create a **poem about the moon**\n",
    "Try different values for temperature and see how it affects the output\n",
    "Try varying frequency_penalty and presence_penalty too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b84185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your moon poem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d837ae91",
   "metadata": {},
   "source": [
    "Below we have written a grading system using another LLM. You will get a grade from A-F along with the reasoning. See if you can adjust your prompt to get a better grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fa64e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Grade: D\n",
       "\n",
       "Explanation:\n",
       "\n",
       "Classic Poetic Structure: The poem employs a simple rhyme scheme and rhythmic pattern, but it lacks formal structure or complexity that would elevate its poetic quality.\n",
       "\n",
       "Non-Repetitiveness: The poem is brief and does not contain redundant ideas or words, so it performs well in this regard.\n",
       "\n",
       "Interesting Content: The content is straightforward and somewhat juvenile. It expresses affection for the moon but doesn't offer deeper insight, imagery, or a captivating narrative element.\n",
       "\n",
       "Creativity: The language and imagery are very basic and cliché. Terms like \"shining bright,\" \"light the way,\" and \"fill my heart with joy and glee\" are common and lack originality, reducing the poem's imaginative appeal.\n",
       "\n",
       "Overall, while the poem is lighthearted and sweet, it lacks the depth, originality, and poetic sophistication deserving of higher grades. It earns a grade of D."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Put a better poem here\n",
    "poem = \"\"\"\n",
    "Oh moonie moonie, shining bright,\n",
    "I love to see you in the night.\n",
    "You light the way for all to see,\n",
    "And fill my heart with joy and glee.\n",
    "\"\"\"\n",
    "\n",
    "grade = utils.grade_poem(client, model, poem)\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(grade))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c625d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero, one, and few-shot prompting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
